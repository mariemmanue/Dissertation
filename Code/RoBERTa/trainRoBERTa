import transformers # from Hugging Face, provides pre-trained transformer models like BERT, GPT
import torch # main PyTorch library for building deep learning models
import torch.nn as nn # The neural network module in PyTorch, used for defining models and layers
import numpy as np
import dataclasses # Python module that provides a decorator and functions for creating simple classes used to store data
from torch.utils.data import Dataset # PyTorch utility to define custom datasets
from typing import List, Union, Dict
import sys # Allows interaction with the system, specifically to read command-line arguments.

# Command-line Arguments
gen_method = sys.argv[1]  # 'CGEdit' or 'CGEdit-ManualGen'
lang = sys.argv[2]  # 'AAE' or 'IndE'

# Model Name Selection
model_name = 'roberta-base'
out_dir = model_name + "_" + gen_method + "_" + lang # defines an output directory name based on the model, generation method, and language

# Multitask Model Definition
class MultitaskModel(transformers.PreTrainedModel): # subclass of transformers.PreTrainedModel for multitask learning
    def __init__(self, encoder, taskmodels_dict):
        super().__init__(transformers.PretrainedConfig()) # Calls the parent class constructor with a generic configuratio
        self.encoder = encoder # shared Transformer model
        self.taskmodels_dict = nn.ModuleDict(taskmodels_dict) # dictionary storing task-specific linear classifiers

# Model Creation Method
    @classmethod # class method to instantiate the model
    def create(cls, model_name, head_type_list):
        """
        Creates each single-feature model (where task == feature), and
        has them share the same encoder transformer.
        """
        taskmodels_dict = {} # Stores individual classification heads
        shared_encoder = transformers.AutoModel.from_pretrained( # Loads a pre-trained BERT model
            model_name,
            config=transformers.AutoConfig.from_pretrained(model_name))

        for task_name in head_type_list:
            head = torch.nn.Linear(768, 2) # Creates a linear layer (768 -> 2) for each task in head_type_list and stores it
            taskmodels_dict[task_name] = head

        return cls(encoder=shared_encoder, taskmodels_dict=taskmodels_dict) # Returns an instance of MultitaskModel.

# Forward Pass
    def forward(self, inputs, **kwargs):
        x = self.encoder(inputs)  # pass thru encoder once, Feeds input tokens through BERT
        x = x.last_hidden_state[:, 0, :]  # get CLS, Extracts the [CLS] token (first token) representation
        out_list = []
        for task_name, head in self.taskmodels_dict.items():  # pass thru each head, Passes [CLS] embedding through each task-specific classifier
            out_list.append(self.taskmodels_dict[task_name](x)) # Stacks outputs into a single tensor
        return torch.vstack(out_list)

# Multitask Trainer
class MultitaskTrainer(transformers.Trainer):
    # Extends transformers.Trainer to compute loss manually, default Trainer assumes a single output per input (single task)
    # dataset stores labels as a 2D tensor: (batch_size, num_tasks), where each row corresponds
    # to an input example and each column corresponds to a different task
    # Since PyTorchâ€™s loss function expects labels in a 1D format, we transpose to reorder dimensions if needed.
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = torch.transpose(inputs["labels"], 0, 1) # Transposes label dimensions and flattens them.
        labels = torch.flatten(labels)
        outputs = model(inputs["input_ids"])  # Forward pass, Feeds input tokens into the model
        loss_fct = nn.CrossEntropyLoss()
        loss = loss_fct(outputs, labels) # Computes loss using cross-entropy between all predictions and their corresponding labels.

        return (loss, outputs) if return_outputs else loss # Returns loss and optionally model outputs.

# Custom Dataset
# not using default because current dataset is tab-separated text data with multiple labels per input, requiring custom preprocessing.
class CustomDataset(Dataset):
    def __init__(self, text, labels): # Stores text and label data.
        self.labels = labels
        self.text = text

    def __len__(self):
        return len(self.labels) # Returns dataset size.

    def __getitem__(self, idx):
        label = self.labels[idx]
        text = self.text[idx]
        sample = {"input_ids": text, "labels": label}
        return sample # Returns a dictionary containing a single example

# Training Function
def trainM(tokenizer, train_f):
    # Load data
    features_dict = {"input_ids": [], "labels": []} # 	Initializes feature storage

    with open(train_f) as r:
        for line in r:
            linesplit = line.strip().split("\t") # Reads a tab-separated file.
            tokenized = tokenizer.encode(linesplit[0], max_length=64, pad_to_max_length=True, truncation=True) # Tokenizes input text.
            features_dict["input_ids"].append(torch.LongTensor(tokenized))
            features_dict["labels"].append(torch.tensor([float(x) for x in linesplit[1:]], dtype=torch.long)) # Converts labels to tensors
# Stacks tensors and wraps them in CustomDataset
    features_dict["input_ids"] = torch.stack(features_dict["input_ids"])
    features_dict["labels"] = torch.stack(features_dict["labels"])
    features_dict = CustomDataset(features_dict["input_ids"], features_dict["labels"])

    # Train, Initializes a trainer with training arguments
    trainer = MultitaskTrainer(
        model=multitask_model,
        args=transformers.TrainingArguments(
            output_dir="./models/" + out_dir,
            overwrite_output_dir=True,
            learning_rate=1e-4,
            do_train=True,
            warmup_steps=300,  # 2 steps per epoch when batch_size=64
            num_train_epochs=500,
            per_device_train_batch_size=64,
            save_steps=500,
        ),
        train_dataset=features_dict,
    )
    trainer.train()
    torch.save({'model_state_dict': multitask_model.state_dict()},
               "./models/" + out_dir + "/final.pt") # Trains the model and saves the final checkpoint


if __name__ == "__main__": # Ensures the script runs only when executed directly

    train_file = "./data/" + gen_method + "/" + lang + ".tsv" # Constructs the training file pat
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # Determines whether to use GPU or CPU

    head_type_list = [ # Defines task types
        "zero-poss",
        "zero-copula",
        "double-tense",
        "be-construction", "resultant-done",
        "finna", "come", "double-modal",
        "multiple-neg", "neg-inversion", "n-inv-neg-concord", "aint",
        "zero-3sg-pres-s", "is-was-gen",
        "zero-pl-s",
        "double-object",
        "wh-qu"]


    multitask_model = MultitaskModel.create(
        model_name=model_name,
        head_type_list=head_type_list
    )
    multitask_model.to(device) # Creates and moves the model to the appropriate device in case GPU is available

    ## Train on contrast set, Loads a tokenizer and starts training
    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)
    trainM(tokenizer, train_file)

